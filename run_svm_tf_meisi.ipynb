{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "\n",
    "#tf値を導出するオブジェクトの設定\n",
    "tf_vectorizer = TfidfVectorizer(input=\"filename\", use_idf=False, smooth_idf=False, max_df=1.0, min_df=1, sublinear_tf=False, norm=\"l2\")\n",
    "\n",
    "\"\"\"１つ目の学習データとテストデータに関して\"\"\"\n",
    "all_file_1=[]\n",
    "all_label_1=[]\n",
    "\n",
    "files_train_1_claim = ['data_set/data_1/train_wakati_toiawase_claim_meisi/claim/' + path for path in os.listdir('data_set/data_1/train_wakati_toiawase_claim_meisi/claim')]\n",
    "files_train_1_toiawase = ['data_set/data_1/train_wakati_toiawase_claim_meisi/toiawase/' + path for path in os.listdir('data_set/data_1/train_wakati_toiawase_claim_meisi/toiawase')]\n",
    "all_file_1.extend(files_train_1_claim)\n",
    "all_file_1.extend(files_train_1_toiawase)\n",
    "\n",
    "\n",
    "label_train_1_claim = np.loadtxt(\"data_set/data_1/train_wakati_toiawase_claim_meisi/train_claim_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "label_train_1_toiawase = np.loadtxt(\"data_set/data_1/train_wakati_toiawase_claim_meisi/train_toiawase_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "all_label_1.extend(label_train_1_claim)\n",
    "all_label_1.extend(label_train_1_toiawase)\n",
    "print(len(all_label_1))\n",
    "\n",
    "files_test_1_claim = ['data_set/data_1/test_wakati_toiawase_claim_meisi/claim/' + path for path in os.listdir('data_set/data_1/test_wakati_toiawase_claim_meisi/claim')]\n",
    "files_test_1_toiawase = ['data_set/data_1/test_wakati_toiawase_claim_meisi/toiawase/' + path for path in os.listdir('data_set/data_1/test_wakati_toiawase_claim_meisi/toiawase')]\n",
    "all_file_1.extend(files_test_1_claim)\n",
    "all_file_1.extend(files_test_1_toiawase)\n",
    "\n",
    "label_test_1_claim = np.loadtxt(\"data_set/data_1/test_wakati_toiawase_claim_meisi/test_claim_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "label_test_1_toiawase = np.loadtxt(\"data_set/data_1/test_wakati_toiawase_claim_meisi/test_toiawase_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "all_label_1.extend(label_test_1_claim)\n",
    "all_label_1.extend(label_test_1_toiawase)\n",
    "\n",
    "tf_1 = tf_vectorizer.fit_transform(all_file_1)\n",
    "print(tf_1)\n",
    "tf_1 = tf_1.toarray()\n",
    "print(tf_1.shape)\n",
    "\n",
    "train_X_1 = tf_1[:7851,::]\n",
    "test_X_1 = tf_1[7851:,::]\n",
    "del tf_1\n",
    "\n",
    "train_y_1 = all_label_1[:7851]\n",
    "test_y_1 = all_label_1[7851:]\n",
    "del all_label_1\n",
    "\n",
    "del files_train_1_claim\n",
    "del files_train_1_toiawase\n",
    "del files_test_1_claim\n",
    "del files_test_1_toiawase\n",
    "# print(train_y_1)\n",
    "# print(test_y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"2つ目の学習データとテストデータに関して\"\"\"\n",
    "all_file_2=[]\n",
    "all_label_2=[]\n",
    "\n",
    "files_train_2_claim = ['data_set/data_2/train_wakati_toiawase_claim_meisi/claim/' + path for path in os.listdir('data_set/data_2/train_wakati_toiawase_claim_meisi/claim')]\n",
    "files_train_2_toiawase = ['data_set/data_2/train_wakati_toiawase_claim_meisi/toiawase/' + path for path in os.listdir('data_set/data_2/train_wakati_toiawase_claim_meisi/toiawase')]\n",
    "all_file_2.extend(files_train_2_claim)\n",
    "all_file_2.extend(files_train_2_toiawase)\n",
    "\n",
    "label_train_2_claim = np.loadtxt(\"data_set/data_2/train_wakati_toiawase_claim_meisi/train_claim_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "label_train_2_toiawase = np.loadtxt(\"data_set/data_2/train_wakati_toiawase_claim_meisi/train_toiawase_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "all_label_2.extend(label_train_2_claim)\n",
    "all_label_2.extend(label_train_2_toiawase)\n",
    "# print(all_label_2)\n",
    "\n",
    "files_test_2_claim = ['data_set/data_2/test_wakati_toiawase_claim_meisi/claim/' + path for path in os.listdir('data_set/data_2/test_wakati_toiawase_claim_meisi/claim')]\n",
    "files_test_2_toiawase = ['data_set/data_2/test_wakati_toiawase_claim_meisi/toiawase/' + path for path in os.listdir('data_set/data_2/test_wakati_toiawase_claim_meisi/toiawase')]\n",
    "all_file_2.extend(files_test_2_claim)\n",
    "all_file_2.extend(files_test_2_toiawase)\n",
    "# print(len(all_file_2))\n",
    "\n",
    "label_test_2_claim = np.loadtxt(\"data_set/data_2/test_wakati_toiawase_claim_meisi/test_claim_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "label_test_2_toiawase = np.loadtxt(\"data_set/data_2/test_wakati_toiawase_claim_meisi/test_toiawase_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "all_label_2.extend(label_test_2_claim)\n",
    "all_label_2.extend(label_test_2_toiawase)\n",
    "# print(len(all_label_2))\n",
    "\n",
    "tf_2 = tf_vectorizer.fit_transform(all_file_2)\n",
    "print(tf_2)\n",
    "tf_2 = tf_2.toarray()\n",
    "print(tf_2.shape)\n",
    "\n",
    "train_X_2 = tf_2[:7851,::]\n",
    "test_X_2 = tf_2[7851:,::]\n",
    "del tf_2\n",
    "\n",
    "train_y_2 = all_label_2[:7851]\n",
    "test_y_2= all_label_2[7851:]\n",
    "del all_label_2\n",
    "# print(train_y_2)\n",
    "# print(test_y_2)\n",
    "del files_train_2_claim\n",
    "del files_train_2_toiawase\n",
    "del files_test_2_claim\n",
    "del files_test_2_toiawase\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"3つ目の学習データとテストデータに関して\"\"\"\n",
    "all_file_3=[]\n",
    "all_label_3=[]\n",
    "\n",
    "files_train_3_claim = ['data_set/data_3/train_wakati_toiawase_claim_meisi/claim/' + path for path in os.listdir('data_set/data_3/train_wakati_toiawase_claim_meisi/claim')]\n",
    "files_train_3_toiawase = ['data_set/data_3/train_wakati_toiawase_claim_meisi/toiawase/' + path for path in os.listdir('data_set/data_3/train_wakati_toiawase_claim_meisi/toiawase')]\n",
    "all_file_3.extend(files_train_3_claim)\n",
    "all_file_3.extend(files_train_3_toiawase)\n",
    "\n",
    "label_train_3_claim = np.loadtxt(\"data_set/data_3/train_wakati_toiawase_claim_meisi/train_claim_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "label_train_3_toiawase = np.loadtxt(\"data_set/data_3/train_wakati_toiawase_claim_meisi/train_toiawase_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "all_label_3.extend(label_train_3_claim)\n",
    "all_label_3.extend(label_train_3_toiawase)\n",
    "# print(all_label_3)\n",
    "\n",
    "files_test_3_claim = ['data_set/data_3/test_wakati_toiawase_claim_meisi/claim/' + path for path in os.listdir('data_set/data_3/test_wakati_toiawase_claim_meisi/claim')]\n",
    "files_test_3_toiawase = ['data_set/data_3/test_wakati_toiawase_claim_meisi/toiawase/' + path for path in os.listdir('data_set/data_3/test_wakati_toiawase_claim_meisi/toiawase')]\n",
    "all_file_3.extend(files_test_3_claim)\n",
    "all_file_3.extend(files_test_3_toiawase)\n",
    "# print(len(all_file_3))\n",
    "\n",
    "label_test_3_claim = np.loadtxt(\"data_set/data_3/test_wakati_toiawase_claim_meisi/test_claim_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "label_test_3_toiawase = np.loadtxt(\"data_set/data_3/test_wakati_toiawase_claim_meisi/test_toiawase_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "all_label_3.extend(label_test_3_claim)\n",
    "all_label_3.extend(label_test_3_toiawase)\n",
    "# print(len(all_label_3))\n",
    "\n",
    "tf_3 = tf_vectorizer.fit_transform(all_file_3)\n",
    "print(tf_3)\n",
    "tf_3 = tf_3.toarray()\n",
    "print(tf_3.shape)\n",
    "\n",
    "train_X_3 = tf_3[:7851,::]\n",
    "test_X_3 = tf_3[7851:,::]\n",
    "del tf_3\n",
    "\n",
    "train_y_3 = all_label_3[:7851]\n",
    "test_y_3 = all_label_3[7851:]\n",
    "del all_label_3\n",
    "# print(train_y_3)\n",
    "# print(test_y_3)\n",
    "del files_train_3_claim\n",
    "del files_train_3_toiawase\n",
    "del files_test_3_claim\n",
    "del files_test_3_toiawase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"4つ目の学習データとテストデータに関して\"\"\"\n",
    "all_file_4=[]\n",
    "all_label_4=[]\n",
    "\n",
    "files_train_4_claim = ['data_set/data_4/train_wakati_toiawase_claim_meisi/claim/' + path for path in os.listdir('data_set/data_4/train_wakati_toiawase_claim_meisi/claim')]\n",
    "files_train_4_toiawase = ['data_set/data_4/train_wakati_toiawase_claim_meisi/toiawase/' + path for path in os.listdir('data_set/data_4/train_wakati_toiawase_claim_meisi/toiawase')]\n",
    "all_file_4.extend(files_train_4_claim)\n",
    "all_file_4.extend(files_train_4_toiawase)\n",
    "\n",
    "label_train_4_claim = np.loadtxt(\"data_set/data_4/train_wakati_toiawase_claim_meisi/train_claim_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "label_train_4_toiawase = np.loadtxt(\"data_set/data_4/train_wakati_toiawase_claim_meisi/train_toiawase_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "all_label_4.extend(label_train_4_claim)\n",
    "all_label_4.extend(label_train_4_toiawase)\n",
    "# print(all_label_4)\n",
    "\n",
    "files_test_4_claim = ['data_set/data_4/test_wakati_toiawase_claim_meisi/claim/' + path for path in os.listdir('data_set/data_4/test_wakati_toiawase_claim_meisi/claim')]\n",
    "files_test_4_toiawase = ['data_set/data_4/test_wakati_toiawase_claim_meisi/toiawase/' + path for path in os.listdir('data_set/data_4/test_wakati_toiawase_claim_meisi/toiawase')]\n",
    "all_file_4.extend(files_test_4_claim)\n",
    "all_file_4.extend(files_test_4_toiawase)\n",
    "# print(len(all_file_4))\n",
    "\n",
    "label_test_4_claim = np.loadtxt(\"data_set/data_4/test_wakati_toiawase_claim_meisi/test_claim_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "label_test_4_toiawase = np.loadtxt(\"data_set/data_4/test_wakati_toiawase_claim_meisi/test_toiawase_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "all_label_4.extend(label_test_4_claim)\n",
    "all_label_4.extend(label_test_4_toiawase)\n",
    "# print(len(all_label_4))\n",
    "\n",
    "tf_4 = tf_vectorizer.fit_transform(all_file_4)\n",
    "print(tf_4)\n",
    "tf_4 = tf_4.toarray()\n",
    "print(tf_4.shape)\n",
    "\n",
    "train_X_4 = tf_4[:7851,::]\n",
    "test_X_4 = tf_4[7851:,::]\n",
    "del tf_4\n",
    "\n",
    "train_y_4 = all_label_4[:7851]\n",
    "test_y_4 = all_label_4[7851:]\n",
    "del all_label_4\n",
    "# print(train_y_4)\n",
    "# print(test_y_4)\n",
    "del files_train_4_claim\n",
    "del files_train_4_toiawase\n",
    "del files_test_4_claim\n",
    "del files_test_4_toiawase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"5つ目の学習データとテストデータに関して\"\"\"\n",
    "all_file_5=[]\n",
    "all_label_5=[]\n",
    "\n",
    "files_train_5_claim = ['data_set/data_5/train_wakati_toiawase_claim_meisi/claim/' + path for path in os.listdir('data_set/data_5/train_wakati_toiawase_claim_meisi/claim')]\n",
    "files_train_5_toiawase = ['data_set/data_5/train_wakati_toiawase_claim_meisi/toiawase/' + path for path in os.listdir('data_set/data_5/train_wakati_toiawase_claim_meisi/toiawase')]\n",
    "all_file_5.extend(files_train_5_claim)\n",
    "all_file_5.extend(files_train_5_toiawase)\n",
    "\n",
    "label_train_5_claim = np.loadtxt(\"data_set/data_5/train_wakati_toiawase_claim_meisi/train_claim_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "label_train_5_toiawase = np.loadtxt(\"data_set/data_5/train_wakati_toiawase_claim_meisi/train_toiawase_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "all_label_5.extend(label_train_5_claim)\n",
    "all_label_5.extend(label_train_5_toiawase)\n",
    "# print(len(all_label_5))\n",
    "\n",
    "files_test_5_claim = ['data_set/data_5/test_wakati_toiawase_claim_meisi/claim/' + path for path in os.listdir('data_set/data_5/test_wakati_toiawase_claim_meisi/claim')]\n",
    "files_test_5_toiawase = ['data_set/data_5/test_wakati_toiawase_claim_meisi/toiawase/' + path for path in os.listdir('data_set/data_5/test_wakati_toiawase_claim_meisi/toiawase')]\n",
    "all_file_5.extend(files_test_5_claim)\n",
    "all_file_5.extend(files_test_5_toiawase)\n",
    "# print(len(all_file_5))\n",
    "\n",
    "label_test_5_claim = np.loadtxt(\"data_set/data_5/test_wakati_toiawase_claim_meisi/test_claim_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "label_test_5_toiawase = np.loadtxt(\"data_set/data_5/test_wakati_toiawase_claim_meisi/test_toiawase_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "all_label_5.extend(label_test_5_claim)\n",
    "all_label_5.extend(label_test_5_toiawase)\n",
    "# print(len(all_label_5))\n",
    "\n",
    "tf_5 = tf_vectorizer.fit_transform(all_file_5)\n",
    "feature_names = tf_vectorizer.get_feature_names()\n",
    "print(tf_5)\n",
    "tf_5 = tf_5.toarray()\n",
    "print(tf_5.shape)\n",
    "\n",
    "train_X_5 = tf_5[:7848,::]\n",
    "test_X_5 = tf_5[7848:,::]\n",
    "del tf_5\n",
    "\n",
    "train_y_5 = all_label_5[:7848]\n",
    "test_y_5 = all_label_5[7848:]\n",
    "del all_label_5\n",
    "\n",
    "# print(train_y_5)\n",
    "# print(test_y_5)\n",
    "print(feature_names)\n",
    "del files_train_5_claim\n",
    "del files_train_5_toiawase\n",
    "del files_test_5_claim\n",
    "del files_test_5_toiawase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "all_accuracy = []\n",
    "all_precision = []\n",
    "all_recall = []\n",
    "all_f1 = []\n",
    "all_confusion = []\n",
    "\n",
    "#svmのインスタンス作成\n",
    "pipe_svc = make_pipeline(SVC(random_state=1))\n",
    "\n",
    "#グリッドサーチの模索範囲指定\n",
    "param_range_gamma = [0.01, 0.001, 0.0001]\n",
    "param_range_C = [10.0, 100.0, 1000.0]\n",
    "param_grid = [{\"svc__C\":param_range_C, \"svc__gamma\":param_range_gamma, \"svc__kernel\":[\"rbf\"]}]\n",
    "\n",
    "#グリッドサーチ内の交差検証を5-層化交差検証にする\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "train_X = [train_X_1, train_X_2, train_X_3, train_X_4, train_X_5]\n",
    "train_y = [train_y_1, train_y_2, train_y_3, train_y_4, train_y_5]\n",
    "test_X = [test_X_1, test_X_2, test_X_3, test_X_4, test_X_5]\n",
    "test_y = [test_y_1, test_y_2, test_y_3, test_y_4, test_y_5]\n",
    "\n",
    "\n",
    "for i in [0, 1, 2, 3, 4]:\n",
    "    \n",
    "    \"\"\"データiに関して\"\"\"\n",
    "    #グリッドサーチのインスタンス生成（f1値で判断）\n",
    "    gs = GridSearchCV(estimator=pipe_svc,\n",
    "                      param_grid=param_grid,\n",
    "                      scoring=\"f1\",\n",
    "                      cv=kf,\n",
    "                      n_jobs=-1)\n",
    "\n",
    "    #学習データiでグリッドサーチ\n",
    "    gs = gs.fit(train_X[i], train_y[i])\n",
    "    print(\"data_%d::best_score: %s\" %(i+1, gs.best_score_))\n",
    "    print(\"data_%d::best_params: %s\" %(i+1, gs.best_params_))\n",
    "    #最適パラメータモデルの取得\n",
    "    clf = gs.best_estimator_\n",
    "    #最適パラメータで学習\n",
    "    clf.fit(train_X[i], train_y[i])\n",
    "    #テストデータ1におけるラベルの予測\n",
    "    pre_label_test = clf.predict(test_X[i])\n",
    "\n",
    "    #テストデータ1におけるそれぞれの評価値をリストに追加\n",
    "    all_accuracy.append(accuracy_score(test_y[i], pre_label_test))\n",
    "    all_precision.append(precision_score(test_y[i], pre_label_test))\n",
    "    all_recall.append(recall_score(test_y[i], pre_label_test))\n",
    "    all_f1.append(f1_score(test_y[i], pre_label_test))\n",
    "    all_confusion.append(confusion_matrix(test_y[i], pre_label_test))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "#5個のデータセットにおけるそれぞれの評価の平均\n",
    "print(\"accuracy: %.3f +- %.3f\" %(np.mean(all_accuracy), np.std(all_accuracy)))\n",
    "print(\"precision: %.3f +- %.3f\" %(np.mean(all_precision), np.std(all_precision)))\n",
    "print(\"recall: %.3f +- %.3f\" %(np.mean(all_recall), np.std(all_recall)))\n",
    "print(\"f1: %.3f +- %.3f\" %(np.mean(all_f1), np.std(all_f1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
