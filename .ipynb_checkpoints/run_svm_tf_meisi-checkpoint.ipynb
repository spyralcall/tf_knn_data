{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "\n",
    "#tf値を導出するオブジェクトの設定\n",
    "tf_vectorizer = TfidfVectorizer(input=\"filename\", use_idf=False, smooth_idf=False, max_df=1.0, min_df=1, sublinear_tf=False, norm=\"l2\")\n",
    "\n",
    "\"\"\"１つ目の学習データとテストデータに関して\"\"\"\n",
    "all_file_1=[]\n",
    "all_label_1=[]\n",
    "\n",
    "files_train_1_claim = ['data_set/data_1/train_wakati_toiawase_claim_meisi/claim/' + path for path in os.listdir('data_set/data_1/train_wakati_toiawase_claim_meisi/claim')]\n",
    "files_train_1_toiawase = ['data_set/data_1/train_wakati_toiawase_claim_meisi/toiawase/' + path for path in os.listdir('data_set/data_1/train_wakati_toiawase_claim_meisi/toiawase')]\n",
    "all_file_1.extend(files_train_1_claim)\n",
    "all_file_1.extend(files_train_1_toiawase)\n",
    "\n",
    "\n",
    "label_train_1_claim = np.loadtxt(\"data_set/data_1/train_wakati_toiawase_claim_meisi/train_claim_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "label_train_1_toiawase = np.loadtxt(\"data_set/data_1/train_wakati_toiawase_claim_meisi/train_toiawase_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "all_label_1.extend(label_train_1_claim)\n",
    "all_label_1.extend(label_train_1_toiawase)\n",
    "print(len(all_label_1))\n",
    "\n",
    "files_test_1_claim = ['data_set/data_1/test_wakati_toiawase_claim_meisi/claim/' + path for path in os.listdir('data_set/data_1/test_wakati_toiawase_claim_meisi/claim')]\n",
    "files_test_1_toiawase = ['data_set/data_1/test_wakati_toiawase_claim_meisi/toiawase/' + path for path in os.listdir('data_set/data_1/test_wakati_toiawase_claim_meisi/toiawase')]\n",
    "all_file_1.extend(files_test_1_claim)\n",
    "all_file_1.extend(files_test_1_toiawase)\n",
    "\n",
    "label_test_1_claim = np.loadtxt(\"data_set/data_1/test_wakati_toiawase_claim_meisi/test_claim_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "label_test_1_toiawase = np.loadtxt(\"data_set/data_1/test_wakati_toiawase_claim_meisi/test_toiawase_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "all_label_1.extend(label_test_1_claim)\n",
    "all_label_1.extend(label_test_1_toiawase)\n",
    "\n",
    "tf_1 = tf_vectorizer.fit_transform(all_file_1)\n",
    "print(tf_1)\n",
    "tf_1 = tf_1.toarray()\n",
    "print(tf_1.shape)\n",
    "\n",
    "train_X_1 = tf_1[:7851,::]\n",
    "test_X_1 = tf_1[7851:,::]\n",
    "del tf_1\n",
    "\n",
    "train_y_1 = all_label_1[:7851]\n",
    "test_y_1 = all_label_1[7851:]\n",
    "del all_label_1\n",
    "\n",
    "del files_train_1_claim\n",
    "del files_train_1_toiawase\n",
    "del files_test_1_claim\n",
    "del files_test_1_toiawase\n",
    "# print(train_y_1)\n",
    "# print(test_y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"2つ目の学習データとテストデータに関して\"\"\"\n",
    "all_file_2=[]\n",
    "all_label_2=[]\n",
    "\n",
    "files_train_2_claim = ['data_set/data_2/train_wakati_toiawase_claim_meisi/claim/' + path for path in os.listdir('data_set/data_2/train_wakati_toiawase_claim_meisi/claim')]\n",
    "files_train_2_toiawase = ['data_set/data_2/train_wakati_toiawase_claim_meisi/toiawase/' + path for path in os.listdir('data_set/data_2/train_wakati_toiawase_claim_meisi/toiawase')]\n",
    "all_file_2.extend(files_train_2_claim)\n",
    "all_file_2.extend(files_train_2_toiawase)\n",
    "\n",
    "label_train_2_claim = np.loadtxt(\"data_set/data_2/train_wakati_toiawase_claim_meisi/train_claim_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "label_train_2_toiawase = np.loadtxt(\"data_set/data_2/train_wakati_toiawase_claim_meisi/train_toiawase_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "all_label_2.extend(label_train_2_claim)\n",
    "all_label_2.extend(label_train_2_toiawase)\n",
    "# print(all_label_2)\n",
    "\n",
    "files_test_2_claim = ['data_set/data_2/test_wakati_toiawase_claim_meisi/claim/' + path for path in os.listdir('data_set/data_2/test_wakati_toiawase_claim_meisi/claim')]\n",
    "files_test_2_toiawase = ['data_set/data_2/test_wakati_toiawase_claim_meisi/toiawase/' + path for path in os.listdir('data_set/data_2/test_wakati_toiawase_claim_meisi/toiawase')]\n",
    "all_file_2.extend(files_test_2_claim)\n",
    "all_file_2.extend(files_test_2_toiawase)\n",
    "# print(len(all_file_2))\n",
    "\n",
    "label_test_2_claim = np.loadtxt(\"data_set/data_2/test_wakati_toiawase_claim_meisi/test_claim_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "label_test_2_toiawase = np.loadtxt(\"data_set/data_2/test_wakati_toiawase_claim_meisi/test_toiawase_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "all_label_2.extend(label_test_2_claim)\n",
    "all_label_2.extend(label_test_2_toiawase)\n",
    "# print(len(all_label_2))\n",
    "\n",
    "tf_2 = tf_vectorizer.fit_transform(all_file_2)\n",
    "print(tf_2)\n",
    "tf_2 = tf_2.toarray()\n",
    "print(tf_2.shape)\n",
    "\n",
    "train_X_2 = tf_2[:7851,::]\n",
    "test_X_2 = tf_2[7851:,::]\n",
    "del tf_2\n",
    "\n",
    "train_y_2 = all_label_2[:7851]\n",
    "test_y_2= all_label_2[7851:]\n",
    "del all_label_2\n",
    "# print(train_y_2)\n",
    "# print(test_y_2)\n",
    "del files_train_2_claim\n",
    "del files_train_2_toiawase\n",
    "del files_test_2_claim\n",
    "del files_test_2_toiawase\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"3つ目の学習データとテストデータに関して\"\"\"\n",
    "all_file_3=[]\n",
    "all_label_3=[]\n",
    "\n",
    "files_train_3_claim = ['data_set/data_3/train_wakati_toiawase_claim_meisi/claim/' + path for path in os.listdir('data_set/data_3/train_wakati_toiawase_claim_meisi/claim')]\n",
    "files_train_3_toiawase = ['data_set/data_3/train_wakati_toiawase_claim_meisi/toiawase/' + path for path in os.listdir('data_set/data_3/train_wakati_toiawase_claim_meisi/toiawase')]\n",
    "all_file_3.extend(files_train_3_claim)\n",
    "all_file_3.extend(files_train_3_toiawase)\n",
    "\n",
    "label_train_3_claim = np.loadtxt(\"data_set/data_3/train_wakati_toiawase_claim_meisi/train_claim_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "label_train_3_toiawase = np.loadtxt(\"data_set/data_3/train_wakati_toiawase_claim_meisi/train_toiawase_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "all_label_3.extend(label_train_3_claim)\n",
    "all_label_3.extend(label_train_3_toiawase)\n",
    "# print(all_label_3)\n",
    "\n",
    "files_test_3_claim = ['data_set/data_3/test_wakati_toiawase_claim_meisi/claim/' + path for path in os.listdir('data_set/data_3/test_wakati_toiawase_claim_meisi/claim')]\n",
    "files_test_3_toiawase = ['data_set/data_3/test_wakati_toiawase_claim_meisi/toiawase/' + path for path in os.listdir('data_set/data_3/test_wakati_toiawase_claim_meisi/toiawase')]\n",
    "all_file_3.extend(files_test_3_claim)\n",
    "all_file_3.extend(files_test_3_toiawase)\n",
    "# print(len(all_file_3))\n",
    "\n",
    "label_test_3_claim = np.loadtxt(\"data_set/data_3/test_wakati_toiawase_claim_meisi/test_claim_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "label_test_3_toiawase = np.loadtxt(\"data_set/data_3/test_wakati_toiawase_claim_meisi/test_toiawase_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "all_label_3.extend(label_test_3_claim)\n",
    "all_label_3.extend(label_test_3_toiawase)\n",
    "# print(len(all_label_3))\n",
    "\n",
    "tf_3 = tf_vectorizer.fit_transform(all_file_3)\n",
    "print(tf_3)\n",
    "tf_3 = tf_3.toarray()\n",
    "print(tf_3.shape)\n",
    "\n",
    "train_X_3 = tf_3[:7851,::]\n",
    "test_X_3 = tf_3[7851:,::]\n",
    "del tf_3\n",
    "\n",
    "train_y_3 = all_label_3[:7851]\n",
    "test_y_3 = all_label_3[7851:]\n",
    "del all_label_3\n",
    "# print(train_y_3)\n",
    "# print(test_y_3)\n",
    "del files_train_3_claim\n",
    "del files_train_3_toiawase\n",
    "del files_test_3_claim\n",
    "del files_test_3_toiawase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"4つ目の学習データとテストデータに関して\"\"\"\n",
    "all_file_4=[]\n",
    "all_label_4=[]\n",
    "\n",
    "files_train_4_claim = ['data_set/data_4/train_wakati_toiawase_claim_meisi/claim/' + path for path in os.listdir('data_set/data_4/train_wakati_toiawase_claim_meisi/claim')]\n",
    "files_train_4_toiawase = ['data_set/data_4/train_wakati_toiawase_claim_meisi/toiawase/' + path for path in os.listdir('data_set/data_4/train_wakati_toiawase_claim_meisi/toiawase')]\n",
    "all_file_4.extend(files_train_4_claim)\n",
    "all_file_4.extend(files_train_4_toiawase)\n",
    "\n",
    "label_train_4_claim = np.loadtxt(\"data_set/data_4/train_wakati_toiawase_claim_meisi/train_claim_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "label_train_4_toiawase = np.loadtxt(\"data_set/data_4/train_wakati_toiawase_claim_meisi/train_toiawase_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "all_label_4.extend(label_train_4_claim)\n",
    "all_label_4.extend(label_train_4_toiawase)\n",
    "# print(all_label_4)\n",
    "\n",
    "files_test_4_claim = ['data_set/data_4/test_wakati_toiawase_claim_meisi/claim/' + path for path in os.listdir('data_set/data_4/test_wakati_toiawase_claim_meisi/claim')]\n",
    "files_test_4_toiawase = ['data_set/data_4/test_wakati_toiawase_claim_meisi/toiawase/' + path for path in os.listdir('data_set/data_4/test_wakati_toiawase_claim_meisi/toiawase')]\n",
    "all_file_4.extend(files_test_4_claim)\n",
    "all_file_4.extend(files_test_4_toiawase)\n",
    "# print(len(all_file_4))\n",
    "\n",
    "label_test_4_claim = np.loadtxt(\"data_set/data_4/test_wakati_toiawase_claim_meisi/test_claim_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "label_test_4_toiawase = np.loadtxt(\"data_set/data_4/test_wakati_toiawase_claim_meisi/test_toiawase_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "all_label_4.extend(label_test_4_claim)\n",
    "all_label_4.extend(label_test_4_toiawase)\n",
    "# print(len(all_label_4))\n",
    "\n",
    "tf_4 = tf_vectorizer.fit_transform(all_file_4)\n",
    "print(tf_4)\n",
    "tf_4 = tf_4.toarray()\n",
    "print(tf_4.shape)\n",
    "\n",
    "train_X_4 = tf_4[:7851,::]\n",
    "test_X_4 = tf_4[7851:,::]\n",
    "del tf_4\n",
    "\n",
    "train_y_4 = all_label_4[:7851]\n",
    "test_y_4 = all_label_4[7851:]\n",
    "del all_label_4\n",
    "# print(train_y_4)\n",
    "# print(test_y_4)\n",
    "del files_train_4_claim\n",
    "del files_train_4_toiawase\n",
    "del files_test_4_claim\n",
    "del files_test_4_toiawase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"5つ目の学習データとテストデータに関して\"\"\"\n",
    "all_file_5=[]\n",
    "all_label_5=[]\n",
    "\n",
    "files_train_5_claim = ['data_set/data_5/train_wakati_toiawase_claim_meisi/claim/' + path for path in os.listdir('data_set/data_5/train_wakati_toiawase_claim_meisi/claim')]\n",
    "files_train_5_toiawase = ['data_set/data_5/train_wakati_toiawase_claim_meisi/toiawase/' + path for path in os.listdir('data_set/data_5/train_wakati_toiawase_claim_meisi/toiawase')]\n",
    "all_file_5.extend(files_train_5_claim)\n",
    "all_file_5.extend(files_train_5_toiawase)\n",
    "\n",
    "label_train_5_claim = np.loadtxt(\"data_set/data_5/train_wakati_toiawase_claim_meisi/train_claim_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "label_train_5_toiawase = np.loadtxt(\"data_set/data_5/train_wakati_toiawase_claim_meisi/train_toiawase_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "all_label_5.extend(label_train_5_claim)\n",
    "all_label_5.extend(label_train_5_toiawase)\n",
    "# print(len(all_label_5))\n",
    "\n",
    "files_test_5_claim = ['data_set/data_5/test_wakati_toiawase_claim_meisi/claim/' + path for path in os.listdir('data_set/data_5/test_wakati_toiawase_claim_meisi/claim')]\n",
    "files_test_5_toiawase = ['data_set/data_5/test_wakati_toiawase_claim_meisi/toiawase/' + path for path in os.listdir('data_set/data_5/test_wakati_toiawase_claim_meisi/toiawase')]\n",
    "all_file_5.extend(files_test_5_claim)\n",
    "all_file_5.extend(files_test_5_toiawase)\n",
    "# print(len(all_file_5))\n",
    "\n",
    "label_test_5_claim = np.loadtxt(\"data_set/data_5/test_wakati_toiawase_claim_meisi/test_claim_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "label_test_5_toiawase = np.loadtxt(\"data_set/data_5/test_wakati_toiawase_claim_meisi/test_toiawase_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "all_label_5.extend(label_test_5_claim)\n",
    "all_label_5.extend(label_test_5_toiawase)\n",
    "# print(len(all_label_5))\n",
    "\n",
    "tf_5 = tf_vectorizer.fit_transform(all_file_5)\n",
    "feature_names = tf_vectorizer.get_feature_names()\n",
    "print(tf_5)\n",
    "tf_5 = tf_5.toarray()\n",
    "print(tf_5.shape)\n",
    "\n",
    "train_X_5 = tf_5[:7848,::]\n",
    "test_X_5 = tf_5[7848:,::]\n",
    "del tf_5\n",
    "\n",
    "train_y_5 = all_label_5[:7848]\n",
    "test_y_5 = all_label_5[7848:]\n",
    "del all_label_5\n",
    "\n",
    "# print(train_y_5)\n",
    "# print(test_y_5)\n",
    "print(feature_names)\n",
    "del files_train_5_claim\n",
    "del files_train_5_toiawase\n",
    "del files_test_5_claim\n",
    "del files_test_5_toiawase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
