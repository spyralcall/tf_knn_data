{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#tf値を導出するオブジェクトの設定\n",
    "tf_vectorizer = TfidfVectorizer(input=\"filename\", use_idf=None, smooth_idf=None, max_df=1.0, min_df=1, sublinear_tf=False, norm=None)\n",
    "\n",
    "\n",
    "\"\"\"１つ目の学習データとテストデータに関して\"\"\"\n",
    "all_file_1=[]\n",
    "all_label_1=[]\n",
    "\n",
    "files_train_1_claim = ['data_set/data_1/train_wakati_toiawase_claim_meisi/claim/' + path for path in os.listdir('data_set/data_1/train_wakati_toiawase_claim_meisi/claim')]\n",
    "files_train_1_toiawase = ['data_set/data_1/train_wakati_toiawase_claim_meisi/toiawase/' + path for path in os.listdir('data_set/data_1/train_wakati_toiawase_claim_meisi/toiawase')]\n",
    "all_file_1.extend(files_train_1_claim)\n",
    "all_file_1.extend(files_train_1_toiawase)\n",
    "\n",
    "label_train_1_claim = np.loadtxt(\"data_set/data_1/train_wakati_toiawase_claim_meisi/train_claim_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "label_train_1_toiawase = np.loadtxt(\"data_set/data_1/train_wakati_toiawase_claim_meisi/train_toiawase_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "all_label_1.extend(label_train_1_claim)\n",
    "all_label_1.extend(label_train_1_toiawase)\n",
    "print(len(all_label_1))\n",
    "\n",
    "files_test_1_claim = ['data_set/data_1/test_wakati_toiawase_claim_meisi/claim/' + path for path in os.listdir('data_set/data_1/test_wakati_toiawase_claim_meisi/claim')]\n",
    "files_test_1_toiawase = ['data_set/data_1/test_wakati_toiawase_claim_meisi/toiawase/' + path for path in os.listdir('data_set/data_1/test_wakati_toiawase_claim_meisi/toiawase')]\n",
    "all_file_1.extend(files_test_1_claim)\n",
    "all_file_1.extend(files_test_1_toiawase)\n",
    "print(len(all_file_1))\n",
    "\n",
    "label_test_1_claim = np.loadtxt(\"data_set/data_1/test_wakati_toiawase_claim_meisi/test_claim_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "label_test_1_toiawase = np.loadtxt(\"data_set/data_1/test_wakati_toiawase_claim_meisi/test_toiawase_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "all_label_1.extend(label_test_1_claim)\n",
    "all_label_1.extend(label_test_1_toiawase)\n",
    "print(len(all_label_1))\n",
    "\n",
    "tf_1 = tf_vectorizer.fit_transform(all_file_1)\n",
    "tf_1 = tf_1.toarray()\n",
    "print(tf_1.shape)\n",
    "\n",
    "train_X_1 = tf_1[:7851,::]\n",
    "test_X_1 = tf_1[7851:,::]\n",
    "\n",
    "train_y_1 = all_label_1[:7851]\n",
    "test_y_1 = all_label_1[7851:]\n",
    "print(train_y_1)\n",
    "print(test_y_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"2つ目の学習データとテストデータに関して\"\"\"\n",
    "all_file_2=[]\n",
    "all_label_2=[]\n",
    "\n",
    "files_train_2_claim = ['data_set/data_2/train_wakati_toiawase_claim_meisi/claim/' + path for path in os.listdir('data_set/data_2/train_wakati_toiawase_claim_meisi/claim')]\n",
    "files_train_2_toiawase = ['data_set/data_2/train_wakati_toiawase_claim_meisi/toiawase/' + path for path in os.listdir('data_set/data_2/train_wakati_toiawase_claim_meisi/toiawase')]\n",
    "all_file_2.extend(files_train_2_claim)\n",
    "all_file_2.extend(files_train_2_toiawase)\n",
    "\n",
    "label_train_2_claim = np.loadtxt(\"data_set/data_2/train_wakati_toiawase_claim_meisi/train_claim_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "label_train_2_toiawase = np.loadtxt(\"data_set/data_2/train_wakati_toiawase_claim_meisi/train_toiawase_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "all_label_2.extend(label_train_2_claim)\n",
    "all_label_2.extend(label_train_2_toiawase)\n",
    "# print(all_label_2)\n",
    "\n",
    "files_test_2_claim = ['data_set/data_2/test_wakati_toiawase_claim_meisi/claim/' + path for path in os.listdir('data_set/data_2/test_wakati_toiawase_claim_meisi/claim')]\n",
    "files_test_2_toiawase = ['data_set/data_2/test_wakati_toiawase_claim_meisi/toiawase/' + path for path in os.listdir('data_set/data_2/test_wakati_toiawase_claim_meisi/toiawase')]\n",
    "all_file_2.extend(files_test_2_claim)\n",
    "all_file_2.extend(files_test_2_toiawase)\n",
    "print(len(all_file_2))\n",
    "\n",
    "label_test_2_claim = np.loadtxt(\"data_set/data_2/test_wakati_toiawase_claim_meisi/test_claim_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "label_test_2_toiawase = np.loadtxt(\"data_set/data_2/test_wakati_toiawase_claim_meisi/test_toiawase_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "all_label_2.extend(label_test_2_claim)\n",
    "all_label_2.extend(label_test_2_toiawase)\n",
    "print(len(all_label_2))\n",
    "\n",
    "tf_2 = tf_vectorizer.fit_transform(all_file_2)\n",
    "tf_2 = tf_2.toarray()\n",
    "print(tf_2.shape)\n",
    "\n",
    "train_X_2 = tf_2[:7851,::]\n",
    "test_X_2 = tf_2[7851:,::]\n",
    "\n",
    "train_y_2 = all_label_2[:7851]\n",
    "test_y_2= all_label_2[7851:]\n",
    "print(train_y_2)\n",
    "print(test_y_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"3つ目の学習データとテストデータに関して\"\"\"\n",
    "all_file_3=[]\n",
    "all_label_3=[]\n",
    "\n",
    "files_train_3_claim = ['data_set/data_3/train_wakati_toiawase_claim_meisi/claim/' + path for path in os.listdir('data_set/data_3/train_wakati_toiawase_claim_meisi/claim')]\n",
    "files_train_3_toiawase = ['data_set/data_3/train_wakati_toiawase_claim_meisi/toiawase/' + path for path in os.listdir('data_set/data_3/train_wakati_toiawase_claim_meisi/toiawase')]\n",
    "all_file_3.extend(files_train_3_claim)\n",
    "all_file_3.extend(files_train_3_toiawase)\n",
    "\n",
    "label_train_3_claim = np.loadtxt(\"data_set/data_3/train_wakati_toiawase_claim_meisi/train_claim_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "label_train_3_toiawase = np.loadtxt(\"data_set/data_3/train_wakati_toiawase_claim_meisi/train_toiawase_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "all_label_3.extend(label_train_3_claim)\n",
    "all_label_3.extend(label_train_3_toiawase)\n",
    "# print(all_label_3)\n",
    "\n",
    "files_test_3_claim = ['data_set/data_3/test_wakati_toiawase_claim_meisi/claim/' + path for path in os.listdir('data_set/data_3/test_wakati_toiawase_claim_meisi/claim')]\n",
    "files_test_3_toiawase = ['data_set/data_3/test_wakati_toiawase_claim_meisi/toiawase/' + path for path in os.listdir('data_set/data_3/test_wakati_toiawase_claim_meisi/toiawase')]\n",
    "all_file_3.extend(files_test_3_claim)\n",
    "all_file_3.extend(files_test_3_toiawase)\n",
    "print(len(all_file_3))\n",
    "\n",
    "label_test_3_claim = np.loadtxt(\"data_set/data_3/test_wakati_toiawase_claim_meisi/test_claim_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "label_test_3_toiawase = np.loadtxt(\"data_set/data_3/test_wakati_toiawase_claim_meisi/test_toiawase_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "all_label_3.extend(label_test_3_claim)\n",
    "all_label_3.extend(label_test_3_toiawase)\n",
    "print(len(all_label_3))\n",
    "\n",
    "tf_3 = tf_vectorizer.fit_transform(all_file_3)\n",
    "tf_3 = tf_3.toarray()\n",
    "print(tf_3.shape)\n",
    "\n",
    "train_X_3 = tf_3[:7851,::]\n",
    "test_X_3 = tf_3[7851:,::]\n",
    "\n",
    "train_y_3 = all_label_3[:7851]\n",
    "test_y_3 = all_label_3[7851:]\n",
    "print(train_y_3)\n",
    "print(test_y_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"4つ目の学習データとテストデータに関して\"\"\"\n",
    "all_file_4=[]\n",
    "all_label_4=[]\n",
    "\n",
    "files_train_4_claim = ['data_set/data_4/train_wakati_toiawase_claim_meisi/claim/' + path for path in os.listdir('data_set/data_4/train_wakati_toiawase_claim_meisi/claim')]\n",
    "files_train_4_toiawase = ['data_set/data_4/train_wakati_toiawase_claim_meisi/toiawase/' + path for path in os.listdir('data_set/data_4/train_wakati_toiawase_claim_meisi/toiawase')]\n",
    "all_file_4.extend(files_train_4_claim)\n",
    "all_file_4.extend(files_train_4_toiawase)\n",
    "\n",
    "label_train_4_claim = np.loadtxt(\"data_set/data_4/train_wakati_toiawase_claim_meisi/train_claim_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "label_train_4_toiawase = np.loadtxt(\"data_set/data_4/train_wakati_toiawase_claim_meisi/train_toiawase_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "all_label_4.extend(label_train_4_claim)\n",
    "all_label_4.extend(label_train_4_toiawase)\n",
    "# print(all_label_4)\n",
    "\n",
    "files_test_4_claim = ['data_set/data_4/test_wakati_toiawase_claim_meisi/claim/' + path for path in os.listdir('data_set/data_4/test_wakati_toiawase_claim_meisi/claim')]\n",
    "files_test_4_toiawase = ['data_set/data_4/test_wakati_toiawase_claim_meisi/toiawase/' + path for path in os.listdir('data_set/data_4/test_wakati_toiawase_claim_meisi/toiawase')]\n",
    "all_file_4.extend(files_test_4_claim)\n",
    "all_file_4.extend(files_test_4_toiawase)\n",
    "print(len(all_file_4))\n",
    "\n",
    "label_test_4_claim = np.loadtxt(\"data_set/data_4/test_wakati_toiawase_claim_meisi/test_claim_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "label_test_4_toiawase = np.loadtxt(\"data_set/data_4/test_wakati_toiawase_claim_meisi/test_toiawase_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "all_label_4.extend(label_test_4_claim)\n",
    "all_label_4.extend(label_test_4_toiawase)\n",
    "print(len(all_label_4))\n",
    "\n",
    "tf_4 = tf_vectorizer.fit_transform(all_file_4)\n",
    "tf_4 = tf_4.toarray()\n",
    "print(tf_4.shape)\n",
    "\n",
    "train_X_4 = tf_4[:7851,::]\n",
    "test_X_4 = tf_4[7851:,::]\n",
    "\n",
    "train_y_4 = all_label_4[:7851]\n",
    "test_y_4 = all_label_4[7851:]\n",
    "print(train_y_4)\n",
    "print(test_y_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"5つ目の学習データとテストデータに関して\"\"\"\n",
    "all_file_5=[]\n",
    "all_label_5=[]\n",
    "\n",
    "files_train_5_claim = ['data_set/data_5/train_wakati_toiawase_claim_meisi/claim/' + path for path in os.listdir('data_set/data_5/train_wakati_toiawase_claim_meisi/claim')]\n",
    "files_train_5_toiawase = ['data_set/data_5/train_wakati_toiawase_claim_meisi/toiawase/' + path for path in os.listdir('data_set/data_5/train_wakati_toiawase_claim_meisi/toiawase')]\n",
    "all_file_5.extend(files_train_5_claim)\n",
    "all_file_5.extend(files_train_5_toiawase)\n",
    "\n",
    "label_train_5_claim = np.loadtxt(\"data_set/data_5/train_wakati_toiawase_claim_meisi/train_claim_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "label_train_5_toiawase = np.loadtxt(\"data_set/data_5/train_wakati_toiawase_claim_meisi/train_toiawase_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "all_label_5.extend(label_train_5_claim)\n",
    "all_label_5.extend(label_train_5_toiawase)\n",
    "print(len(all_label_5))\n",
    "\n",
    "files_test_5_claim = ['data_set/data_5/test_wakati_toiawase_claim_meisi/claim/' + path for path in os.listdir('data_set/data_5/test_wakati_toiawase_claim_meisi/claim')]\n",
    "files_test_5_toiawase = ['data_set/data_5/test_wakati_toiawase_claim_meisi/toiawase/' + path for path in os.listdir('data_set/data_5/test_wakati_toiawase_claim_meisi/toiawase')]\n",
    "all_file_5.extend(files_test_5_claim)\n",
    "all_file_5.extend(files_test_5_toiawase)\n",
    "print(len(all_file_5))\n",
    "\n",
    "label_test_5_claim = np.loadtxt(\"data_set/data_5/test_wakati_toiawase_claim_meisi/test_claim_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "label_test_5_toiawase = np.loadtxt(\"data_set/data_5/test_wakati_toiawase_claim_meisi/test_toiawase_label.txt\", delimiter=\"\\n\", dtype=float)\n",
    "all_label_5.extend(label_test_5_claim)\n",
    "all_label_5.extend(label_test_5_toiawase)\n",
    "print(len(all_label_5))\n",
    "\n",
    "tf_5 = tf_vectorizer.fit_transform(all_file_5)\n",
    "feature_names = tf_vectorizer.get_feature_names()\n",
    "tf_5 = tf_5.toarray()\n",
    "print(tf_5.shape)\n",
    "\n",
    "train_X_5 = tf_5[:7848,::]\n",
    "test_X_5 = tf_5[7848:,::]\n",
    "\n",
    "train_y_5 = all_label_5[:7848]\n",
    "test_y_5 = all_label_5[7848:]\n",
    "print(train_y_5)\n",
    "print(test_y_5)\n",
    "# print(feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "all_accuracy = []\n",
    "all_precision = []\n",
    "all_recall = []\n",
    "all_f1 = []\n",
    "all_confusion = []\n",
    "\n",
    "#k近傍法のインスタンス作成\n",
    "knn = KNeighborsClassifier(p=2, metric=\"minkowski\")\n",
    "\n",
    "#グリッドサーチの模索範囲指定\n",
    "param_range = [1,2,3,4,5,6,7,8,9,10]\n",
    "param_grid = [{\"n_neighbors\":param_range, \"algorithm\":[\"ball_tree\"], \"leaf_size\":[30]},\n",
    "              {\"n_neighbors\":param_range, \"algorithm\":[\"kd_tree\"], \"leaf_size\":[30]},\n",
    "              {\"n_neighbors\":param_range, \"algorithm\":[\"brute\"]}]\n",
    "\n",
    "#グリッドサーチ内の交差検証を5-層化交差検証にする\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "smote = SMOTE(\"minority\", random_state=2)\n",
    "\n",
    "\"\"\"データ１に関して\"\"\"\n",
    "#SMOTEENNを行い新しい訓練データを作成\n",
    "train_X_1_smote , train_y_1_smote = smote.fit_sample(train_X_1, train_y_1)\n",
    "print(\"smote_train_1(label=1): %s\" %train_X_1_smote[ train_y_1_smote == 1 ].shape[0])\n",
    "print(\"smote_train_1(label=0): %s\" %train_X_1_smote[ train_y_1_smote == 0 ].shape[0])\n",
    "\n",
    "\n",
    "#グリッドサーチのインスタンス生成（f1値で判断）\n",
    "gs = GridSearchCV(estimator=knn,\n",
    "                  param_grid=param_grid,\n",
    "                  scoring=\"f1\",\n",
    "                  cv=kf,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "#学習データ1でグリッドサーチ\n",
    "gs = gs.fit(train_X_1_smote, train_y_1_smote)\n",
    "print(\"data_1::best_score: %s\" %gs.best_score_)\n",
    "print(\"data_1::best_params: %s\" %gs.best_params_)\n",
    "#最適パラメータモデルの取得\n",
    "clf_1 = gs.best_estimator_\n",
    "#最適パラメータで学習\n",
    "clf_1.fit(train_X_1_smote, train_y_1_smote)\n",
    "#テストデータ1におけるラベルの予測\n",
    "pre_label_test_1 = clf_1.predict(test_X_1)\n",
    "\n",
    "#テストデータ1におけるそれぞれの評価値をリストに追加\n",
    "all_accuracy.append(accuracy_score(test_y_1, pre_label_test_1))\n",
    "all_precision.append(precision_score(test_y_1, pre_label_test_1))\n",
    "all_recall.append(recall_score(test_y_1, pre_label_test_1))\n",
    "all_f1.append(f1_score(test_y_1, pre_label_test_1))\n",
    "all_confusion.append(confusion_matrix(test_y_1, pre_label_test_1))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"データ２に関して\"\"\"\n",
    "#SMOTEENNを行い新しい訓練データを作成\n",
    "train_X_2_smote , train_y_2_smote = smote.fit_sample(train_X_2, train_y_2)\n",
    "print(\"smote_train_2(label=1): %s\" %train_X_2_smote[ train_y_2_smote == 1 ].shape[0])\n",
    "print(\"smote_train_2(label=0): %s\" %train_X_2_smote[ train_y_2_smote == 0 ].shape[0])\n",
    "\n",
    "#グリッドサーチのインスタンス生成（f1値で判断）\n",
    "gs = GridSearchCV(estimator=knn,\n",
    "                  param_grid=param_grid,\n",
    "                  scoring=\"f1\",\n",
    "                  cv=kf,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "#学習データ2でグリッドサーチ\n",
    "gs = gs.fit(train_X_2_smote, train_y_2_smote)\n",
    "print(\"data_2::best_score: %s\" %gs.best_score_)\n",
    "print(\"data_2::best_params: %s\" %gs.best_params_)\n",
    "#最適パラメータモデルの取得\n",
    "clf_2 = gs.best_estimator_\n",
    "#最適パラメータで学習\n",
    "clf_2.fit(train_X_2_smote, train_y_2_smote)\n",
    "#テストデータ２におけるラベルの予測\n",
    "pre_label_test_2 = clf_2.predict(test_X_2)\n",
    "\n",
    "#テストデータ２におけるそれぞれの評価値をリストに追加\n",
    "all_accuracy.append(accuracy_score(test_y_2, pre_label_test_2))\n",
    "all_precision.append(precision_score(test_y_2, pre_label_test_2))\n",
    "all_recall.append(recall_score(test_y_2, pre_label_test_2))\n",
    "all_f1.append(f1_score(test_y_2, pre_label_test_2))\n",
    "all_confusion.append(confusion_matrix(test_y_2, pre_label_test_2))\n",
    "\n",
    "\n",
    "\"\"\"データ3に関して\"\"\"\n",
    "#SMOTEENNを行い新しい訓練データを作成\n",
    "train_X_3_smote , train_y_3_smote = smote.fit_sample(train_X_3, train_y_3)\n",
    "print(\"smote_train_3(label=1): %s\" %train_X_3_smote[ train_y_3_smote == 1 ].shape[0])\n",
    "print(\"smote_train_3(label=0): %s\" %train_X_3_smote[ train_y_3_smote == 0 ].shape[0])\n",
    "\n",
    "#グリッドサーチのインスタンス生成（f1値で判断）\n",
    "gs = GridSearchCV(estimator=knn,\n",
    "                  param_grid=param_grid,\n",
    "                  scoring=\"f1\",\n",
    "                  cv=kf,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "#学習データ３でグリッドサーチ\n",
    "gs = gs.fit(train_X_3_smote, train_y_3_smote)\n",
    "print(\"data_3::best_score: %s\" %gs.best_score_)\n",
    "print(\"data_3::best_params: %s\" %gs.best_params_)\n",
    "#最適パラメータモデルの取得\n",
    "clf_3 = gs.best_estimator_\n",
    "#最適パラメータで学習\n",
    "clf_3.fit(train_X_3_smote, train_y_3_smote)\n",
    "#テストデータ３におけるラベルの予測\n",
    "pre_label_test_3 = clf_3.predict(test_X_3)\n",
    "\n",
    "#テストデータ３におけるそれぞれの評価値をリストに追加\n",
    "all_accuracy.append(accuracy_score(test_y_3, pre_label_test_3))\n",
    "all_precision.append(precision_score(test_y_3, pre_label_test_3))\n",
    "all_recall.append(recall_score(test_y_3, pre_label_test_3))\n",
    "all_f1.append(f1_score(test_y_3, pre_label_test_3))\n",
    "all_confusion.append(confusion_matrix(test_y_3, pre_label_test_3))\n",
    "\n",
    "\n",
    "\"\"\"データ4に関して\"\"\"\n",
    "#SMOTEENNを行い新しい訓練データを作成\n",
    "train_X_4_smote , train_y_4_smote = smote.fit_sample(train_X_4, train_y_4)\n",
    "print(\"smote_train_4(label=1): %s\" %train_X_4_smote[ train_y_4_smote == 1 ].shape[0])\n",
    "print(\"smote_train_4(label=0): %s\" %train_X_4_smote[ train_y_4_smote == 0 ].shape[0])\n",
    "\n",
    "#グリッドサーチのインスタンス生成（f1値で判断）\n",
    "gs = GridSearchCV(estimator=knn,\n",
    "                  param_grid=param_grid,\n",
    "                  scoring=\"f1\",\n",
    "                  cv=kf,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "#学習データ４でグリッドサーチ\n",
    "gs = gs.fit(train_X_4_smote, train_y_4_smote)\n",
    "print(\"data_4::best_score: %s\" %gs.best_score_)\n",
    "print(\"data_4::best_params: %s\" %gs.best_params_)\n",
    "#最適パラメータモデルの取得\n",
    "clf_4 = gs.best_estimator_\n",
    "#最適パラメータで学習\n",
    "clf_4.fit(train_X_4_smote, train_y_4_smote)\n",
    "#テストデータ４におけるラベルの予測\n",
    "pre_label_test_4 = clf_4.predict(test_X_4)\n",
    "\n",
    "#テストデータ４におけるそれぞれの評価値をリストに追加\n",
    "all_accuracy.append(accuracy_score(test_y_4, pre_label_test_4))\n",
    "all_precision.append(precision_score(test_y_4, pre_label_test_4))\n",
    "all_recall.append(recall_score(test_y_4, pre_label_test_4))\n",
    "all_f1.append(f1_score(test_y_4, pre_label_test_4))\n",
    "all_confusion.append(confusion_matrix(test_y_4, pre_label_test_4))\n",
    "\n",
    "\n",
    "\"\"\"データ5に関して\"\"\"\n",
    "#SMOTEENNを行い新しい訓練データを作成\n",
    "train_X_5_smote , train_y_5_smote = smote.fit_sample(train_X_5, train_y_5)\n",
    "print(\"smote_train_5(label=1): %s\" %train_X_5_smote[ train_y_5_smote == 1 ].shape[0])\n",
    "print(\"smote_train_5(label=0): %s\" %train_X_5_smote[ train_y_5_smote == 0 ].shape[0])\n",
    "\n",
    "#グリッドサーチのインスタンス生成（f1値で判断）\n",
    "gs = GridSearchCV(estimator=knn,\n",
    "                  param_grid=param_grid,\n",
    "                  scoring=\"f1\",\n",
    "                  cv=kf,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "#学習データ５でグリッドサーチ\n",
    "gs = gs.fit(train_X_5_smote, train_y_5_smote)\n",
    "print(\"data_5::best_score: %s\" %gs.best_score_)\n",
    "print(\"data_5::best_params: %s\" %gs.best_params_)\n",
    "#最適パラメータモデルの取得\n",
    "clf_5 = gs.best_estimator_\n",
    "#最適パラメータで学習\n",
    "clf_5.fit(train_X_5_smote, train_y_5_smote)\n",
    "#テストデータ5におけるラベルの予測\n",
    "pre_label_test_5 = clf_5.predict(test_X_5)\n",
    "\n",
    "#テストデータ５におけるそれぞれの評価値をリストに追加\n",
    "all_accuracy.append(accuracy_score(test_y_5, pre_label_test_5))\n",
    "all_precision.append(precision_score(test_y_5, pre_label_test_5))\n",
    "all_recall.append(recall_score(test_y_5, pre_label_test_5))\n",
    "all_f1.append(f1_score(test_y_5, pre_label_test_5))\n",
    "all_confusion.append(confusion_matrix(test_y_5, pre_label_test_5))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#5個のデータセットにおけるそれぞれの評価の平均\n",
    "print(\"accuracy: %.3f +- %.3f\" %(np.mean(all_accuracy), np.std(all_accuracy)))\n",
    "print(\"precision: %.3f +- %.3f\" %(np.mean(all_precision), np.std(all_precision)))\n",
    "print(\"recall: %.3f +- %.3f\" %(np.mean(all_recall), np.std(all_recall)))\n",
    "print(\"f1: %.3f +- %.3f\" %(np.mean(all_f1), np.std(all_f1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
